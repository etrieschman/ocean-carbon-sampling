{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e10f8c7-3ea1-4f7b-9410-f34d56d7468b",
   "metadata": {},
   "source": [
    "# Sample design: D-optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaeda72f-adba-4388-8d2f-929b293fdf50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/etriesch/.pyenv/versions/3.9.5/envs/venv.ocean-carbon-sampling/lib/python3.9/site-packages/geopandas/_compat.py:111: UserWarning: The Shapely GEOS version (3.10.2-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import shapely\n",
    "from shapely.geometry import Point, Polygon, MultiPoint, MultiPolygon\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import itertools\n",
    "import scipy\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn import cluster\n",
    "from sklearn import neighbors\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, cross_validate\n",
    "from sklearn import metrics, cluster\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import ShuffleSplit, GroupKFold, cross_validate, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Lasso, lasso_path\n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer, StandardScaler\n",
    "\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0576f4ea-fdbd-4d19-8efe-07b879dca72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 18})\n",
    "SMALL_SIZE = 10\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f9db345-a46f-4053-a944-a8ccdd57a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d9cd16f-8e8f-4e6d-8ce1-9abb4e40a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = Path('/Users/etriesch/dev/ocean-carbon-sampling/')\n",
    "data_clean_path = repo_path / 'data/clean/'\n",
    "data_raw_path = repo_path / 'data/raw/'\n",
    "results_path = repo_path / 'results/'\n",
    "geo_crs = 'epsg:4326'\n",
    "proj_crs = '+proj=cea'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f41a69ff-0584-44ec-aec1-a48004b7d101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in full data and merge\n",
    "t_raw = pd.read_csv(data_clean_path / 'sst.csv')\n",
    "c_raw = pd.read_csv(data_clean_path / 'chlor_a.csv')\n",
    "o_m = pd.merge(left=t_raw, right=c_raw, how='inner', on=['x', 'y'], suffixes=('_t', '_c'))\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(o_m.lon_t, o_m.lat_t)]\n",
    "geo_m = gpd.GeoDataFrame(o_m, geometry=geometry, crs=geo_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cb577d-54d7-4fcd-8a02-9fe9f78fa4b9",
   "metadata": {},
   "source": [
    "# D-optimal design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314985f5-9472-463e-b354-0f2b67206fd5",
   "metadata": {},
   "source": [
    "## Subset to sample zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8012ec9-ab49-460a-8596-2cab6cc0033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load coastlines (saved locally)\n",
    "boundary_fp = data_raw_path / 'stanford-vg541kt0643-shapefile.zip'\n",
    "boundary = gpd.read_file(data_raw_path / boundary_fp).to_crs(geo_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d385a6be-29e2-4b99-b95c-aa7f1ed4ab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monterrey desal mask\n",
    "ca_cent = [-121.788649, 36.802834]\n",
    "ca_lats = [33.48, 39.48]\n",
    "ca_lons = [-125.48, -119.48]\n",
    "# Texas desal mask\n",
    "tx_cent = [-95.311296, 28.927239]\n",
    "tx_lats = [25.57, 31.57]\n",
    "tx_lons = [-98.21, -92.21]\n",
    "# NH desal mask\n",
    "nh_cent = [-70.799678, 42.563588]\n",
    "nh_lats = [39.38, 45.38]\n",
    "nh_lons = [-73.50, -67.50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9734a78d-50e5-46b3-8579-81b4041a33ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make disks\n",
    "BUFFER = 1.5\n",
    "ca_disc = gpd.GeoSeries(Point(ca_cent), crs=proj_crs).buffer(BUFFER).set_crs(geo_crs, allow_override=True)\n",
    "ca_disc = gpd.GeoDataFrame(geometry=ca_disc)\n",
    "tx_disc = gpd.GeoSeries(Point(tx_cent), crs=proj_crs).buffer(BUFFER).set_crs(geo_crs, allow_override=True)\n",
    "tx_disc = gpd.GeoDataFrame(geometry=tx_disc)\n",
    "nh_disc = gpd.GeoSeries(Point(nh_cent), crs=proj_crs).buffer(BUFFER).set_crs(geo_crs, allow_override=True)\n",
    "nh_disc = gpd.GeoDataFrame(geometry=nh_disc)\n",
    "# cut discs at coastal boundary\n",
    "ca = ca_disc.overlay(boundary, how='difference')\n",
    "tx = tx_disc.overlay(boundary, how='difference')\n",
    "nh = nh_disc.overlay(boundary, how='difference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93b406f0-05f5-44cf-a31c-dfb5a08a473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sample zones\n",
    "pac_sample = geo_m.overlay(ca, how='intersection')\n",
    "atl_sample = geo_m.overlay(nh, how='intersection')\n",
    "gul_sample = geo_m.overlay(tx, how='intersection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3af93c3a-166a-4b25-b5df-50909dede68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make tuples of sample zones, discs, and desalination plant locations\n",
    "PAC = [pac_sample, ca, ca_cent] # pacific\n",
    "ATL = [atl_sample, nh, nh_cent] # atlantic\n",
    "GUL = [gul_sample, tx, tx_cent] # gulf\n",
    "\n",
    "sample_locations = [pac_sample, atl_sample, gul_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153534d0-0054-4138-99c0-bdd7adfd0587",
   "metadata": {},
   "source": [
    "## Get D-optimal design points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b3c475c-8247-40ab-9cb1-b6adf9a8abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doptimal_sd(df, response_cols, strategy='uniform', n_bins=10):\n",
    "    # build dataset\n",
    "    # scalarize\n",
    "    scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "    scaler.fit(df[response_cols])\n",
    "    X = pd.DataFrame(scaler.transform(df[response_cols]), columns=response_cols)\n",
    "    # discretize\n",
    "    discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy)\n",
    "    X = pd.DataFrame(discretizer.fit_transform(X))\n",
    "    X.columns = response_cols\n",
    "    X_nodups = X.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # set up convex problem\n",
    "    v = X_nodups.to_numpy()\n",
    "    p = X_nodups.shape[0]\n",
    "    I = np.identity(p)\n",
    "    vvT = [np.outer(v[i,], v[i,]) for i in range(p)] # array of x_i @ x_i.T\n",
    "    l = cp.Variable(p) # variable\n",
    "    constraints = [l >= 0, cp.sum(l) == 1] # constraints\n",
    "    # objective\n",
    "    cost = [l[i] * vvT[i] for i in range(p)]\n",
    "    obj = cp.Minimize(-cp.log_det(cp.sum(cost)))\n",
    "\n",
    "    # Solve for optimal points\n",
    "    prob = cp.Problem(obj, constraints)\n",
    "    prob.solve()\n",
    "    # print('status:', prob.status)\n",
    "    # print('optimal value:', prob.value)\n",
    "\n",
    "    # bring samples back to main dataset\n",
    "    X_nodups.loc[:,'n_samples'] = np.round(200*l.value)\n",
    "    X_samp = pd.merge(left=X, right=X_nodups, how='left', on=response_cols)\n",
    "    X_samp['count'] = X_samp.groupby(response_cols).transform('count')\n",
    "    X_samp['n_samples'] = np.round(X_samp['n_samples'] / X_samp['count'])\n",
    "    df_samp = pd.merge(left=df, right=X_samp.drop(columns=response_cols), left_index=True, right_index=True)\n",
    "    df_samp_sub = df_samp.loc[df_samp.n_samples > 0]\n",
    "    return df_samp_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcca427-5415-4231-817b-3ea475253f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pacific samples: 200.0 198.0 200.0\n",
      "Atlantic samples: 200.0 199.0 203.0\n"
     ]
    }
   ],
   "source": [
    "# universal settings\n",
    "n_bins_width = 20\n",
    "n_bins_freq = 10\n",
    "n_bins_kmeans = 10\n",
    "# response cols selected in lasso path analysis\n",
    "response_cols = ['max_t', 'std_c', 'months_below_mean', 'below_mean']\n",
    "\n",
    "# get samples\n",
    "# PAC\n",
    "pac_eq_width = get_doptimal_sd(pac_sample, response_cols, strategy='uniform', n_bins=n_bins_width)\n",
    "pac_eq_freq = get_doptimal_sd(pac_sample, response_cols, strategy='quantile', n_bins=n_bins_freq)\n",
    "pac_kmeans = get_doptimal_sd(pac_sample, response_cols, strategy='kmeans', n_bins=n_bins_kmeans)\n",
    "print('Pacific samples:', pac_eq_width.n_samples.sum(), pac_eq_freq.n_samples.sum(), pac_kmeans.n_samples.sum())\n",
    "# ATL\n",
    "atl_eq_width = get_doptimal_sd(atl_sample, response_cols, strategy='uniform', n_bins=n_bins_width)\n",
    "atl_eq_freq = get_doptimal_sd(atl_sample, response_cols, strategy='quantile', n_bins=n_bins_freq)\n",
    "atl_kmeans = get_doptimal_sd(atl_sample, response_cols, strategy='kmeans', n_bins=n_bins_kmeans)\n",
    "print('Atlantic samples:', atl_eq_width.n_samples.sum(), atl_eq_freq.n_samples.sum(), atl_kmeans.n_samples.sum())\n",
    "# GUL\n",
    "gul_eq_width = get_doptimal_sd(gul_sample, response_cols, strategy='uniform', n_bins=n_bins_width)\n",
    "gul_eq_freq = get_doptimal_sd(gul_sample, response_cols, strategy='quantile', n_bins=n_bins_freq)\n",
    "gul_kmeans = get_doptimal_sd(gul_sample, response_cols, strategy='kmeans', n_bins=n_bins_kmeans)\n",
    "print('Gulf samples:', gul_eq_width.n_samples.sum(), gul_eq_freq.n_samples.sum(), gul_kmeans.n_samples.sum())\n",
    "\n",
    "\n",
    "PAC = PAC[:3] + [pac_eq_width, pac_eq_freq, pac_kmeans]\n",
    "ATL = ATL[:3] + [atl_eq_width, atl_eq_freq, atl_kmeans]\n",
    "GUL = GUL[:3] + [gul_eq_width, gul_eq_freq, gul_kmeans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50353c8f-d426-499d-88b2-e7141534b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(20,20))\n",
    "\n",
    "# PAC\n",
    "PAC[1].boundary.plot(ax=ax[0], alpha=0.7, color='gray') # background\n",
    "PAC[0].plot(ax=ax[0], column='max_t', alpha=0.3, markersize=20, legend=True, legend_kwds={'shrink': 0.3})\n",
    "PAC[3].plot(ax=ax[0], color='darkred', markersize=PAC[3].n_samples*5, alpha=0.5, label='Uniform disc.') # sample points\n",
    "PAC[4].plot(ax=ax[0], color='darkgoldenrod', markersize=PAC[4].n_samples*5, alpha=0.5, label='Equal freq. disc.') # sample points\n",
    "PAC[5].plot(ax=ax[0], color='darkblue', markersize=PAC[5].n_samples*5, alpha=0.5, label='K-means disc.') # sample points\n",
    "gpd.GeoSeries(Point(PAC[2])).plot(ax=ax[0], color='darkgreen', markersize=300, marker='*', label='Desal. plant') # desalination plant\n",
    "ax[0].set_title(f'Pacific: D-optimal sample design') #title\n",
    "ax[0].legend()\n",
    "# GUL\n",
    "GUL[1].boundary.plot(ax=ax[1], alpha=0.7, color='gray') # background\n",
    "GUL[0].plot(ax=ax[1], column='max_t', alpha=0.3, markersize=20, legend=True, legend_kwds={'shrink': 0.2})\n",
    "GUL[3].plot(ax=ax[1], color='darkred', markersize=GUL[3].n_samples*5, alpha=0.5, label='Uniform disc.') # sample points\n",
    "GUL[4].plot(ax=ax[1], color='darkgoldenrod', markersize=GUL[4].n_samples*5, alpha=0.5, label='Equal freq. disc.') # sample points\n",
    "GUL[5].plot(ax=ax[1], color='darkblue', markersize=GUL[5].n_samples*5, alpha=0.5, label='K-means disc.') # sample points\n",
    "gpd.GeoSeries(Point(GUL[2])).plot(ax=ax[1], color='darkgreen', markersize=300, marker='*', label='Desal. plant') # desalination plant\n",
    "ax[1].set_title(f'Gulf: D-optimal sample design') #title\n",
    "ax[1].legend()\n",
    "\n",
    "# ATL\n",
    "ATL[1].boundary.plot(ax=ax[2], alpha=0.7, color='gray') # background\n",
    "ATL[0].plot(ax=ax[2], column='max_t', alpha=0.3, markersize=20, legend=True, legend_kwds={'shrink': 0.27})\n",
    "ATL[3].plot(ax=ax[2], color='darkred', markersize=ATL[3].n_samples*5, alpha=0.5, label='Uniform disc.') # sample points\n",
    "ATL[4].plot(ax=ax[2], color='darkgoldenrod', markersize=ATL[4].n_samples*5, alpha=0.5, label='Equal freq. disc.') # sample points\n",
    "ATL[5].plot(ax=ax[2], color='darkblue', markersize=ATL[5].n_samples*5, alpha=0.5, label='K-means disc.') # sample points\n",
    "gpd.GeoSeries(Point(ATL[2])).plot(ax=ax[2], color='darkgreen', markersize=300, marker='*', label='Desal. plant') # desalination plant\n",
    "ax[2].set_title(f'Atlantic: D-optimal sample design') #title\n",
    "ax[2].legend()\n",
    "\n",
    "plt.savefig(results_path / 'fig_doptimal.png', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d33b75a-bc36-430a-8255-8601a83620c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebf0ed05-f56f-44bf-bd16-0650db66fb5b",
   "metadata": {},
   "source": [
    "# Run regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d2cab0-7ef5-4eac-aee0-de7b93395b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTERS=5\n",
    "estimator = LinearRegression()\n",
    "params = {}\n",
    "\n",
    "grid_search = GridSearchCV(estimator, param_grid=params, scoring='neg_root_mean_squared_error', cv=GroupKFold(n_splits=CLUSTERS))\n",
    "\n",
    "CLUSTERS = 10\n",
    "cl = cluster.KMeans(n_clusters=CLUSTERS)\n",
    "cl.fit(m_sub.loc[m_sub.pacific, ['x', 'y']])\n",
    "X_pac['kmeans_cluster'] = cl.labels_\n",
    "cl.fit(m_sub.loc[m_sub.pacific==False,['x', 'y']])\n",
    "X_atl['kmeans_cluster'] = cl.labels_\n",
    "\n",
    "\n",
    "cv = grid_search.fit(df[predictor_vars], df[response_var], groups=df['kmeans_cluster'])\n",
    "\n",
    "x_in = X.loc[:, ('pacific', 'month', 'year', 'chlor_a_cln', 'sst_cln', 'months_below_mean', 'max', 'std', 'max_ca', 'std_ca')]\n",
    "\n",
    "grid_search = GridSearchCV(\\n\",\n",
    "    \"            estimator=estimator, \\n\",\n",
    "    \"            param_grid=params,\\n\",\n",
    "    \"            scoring=scoring,\\n\",\n",
    "    \"            refit=False, \\n\",\n",
    "    \"            cv=GroupKFold(n_splits=CLUSTERS), \\n\",\n",
    "    \"            verbose=0, \\n\",\n",
    "    \"            return_train_score=False\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"cv_summ = pd.DataFrame()\\n\",\n",
    "    \"for ocean in [True, False]:\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    df = m_sub.loc[m_sub.pacific==ocean]\\n\",\n",
    "    \"    cl.fit(df[['x', 'y']])\\n\",\n",
    "    \"    df['kmeans_cluster'] = cl.labels_\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    cv = grid_search.fit(df[predictor_vars], df[response_var], groups=df['kmeans_cluster'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"    cv = pd.DataFrame({'pacific': [ocean]*len(cv.cv_results_['params']),\\n\",\n",
    "    \"        'model': [str(estimator)]*len(cv.cv_results_['params']),\\n\",\n",
    "    \"        'params': cv.cv_results_['params'],\\n\",\n",
    "    \"        'mean_test_neg_mean_squared_error': cv.cv_results_['mean_test_neg_mean_squared_error'],\\n\",\n",
    "    \"        'mean_test_r_squared': cv.cv_results_['mean_test_r_squared']}).sort_values('mean_test_neg_mean_squared_error')\\n\",\n",
    "    \"    cv_summ = pd.concat([cv, cv_summ])\\n\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18786b82-8de3-4bed-97cd-6fee9595b1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
